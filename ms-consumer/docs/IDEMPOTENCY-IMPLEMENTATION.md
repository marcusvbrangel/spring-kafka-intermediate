# Idempotency Implementation - Consumer Exactly-Once Semantics

## ğŸ“‹ Overview

This document describes the **idempotency implementation** in the `PaymentApprovedConsumer` to guarantee **exactly-once processing semantics**, preventing duplicate message processing in critical payment flows.

## ğŸ¯ Problem Being Solved

Without idempotency, the following scenarios can lead to **duplicate processing**:

1. **Consumer crash after processing but before offset commit**
   - Message is processed (e.g., payment credited)
   - Consumer crashes before calling `acknowledgment.acknowledge()`
   - On restart, Kafka redelivers the message â†’ **duplicate processing**

2. **Network issues during commit**
   - Processing succeeds
   - Kafka commit times out due to network issue
   - Consumer retries â†’ **duplicate processing**

3. **Consumer rebalancing**
   - Processing in progress
   - Rebalancing occurs, message reassigned to another consumer
   - Both consumers process the same message â†’ **duplicate processing**

4. **Manual retries from DLQ**
   - Message sent to DLQ
   - Admin reprocesses from DLQ
   - Message already processed â†’ **duplicate processing**

**In payment systems, duplicate processing can mean:**
- âŒ Charging customer twice
- âŒ Crediting merchant twice
- âŒ Incorrect financial reports
- âŒ Compliance violations

## âœ… Solution: Idempotency Pattern

The implementation uses a **database-backed idempotency check** to track processed events.

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PaymentApprovedConsumer.handlePaymentApproved()                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  1. Check if event already processed                           â”‚
â”‚     â†“                                                           â”‚
â”‚     SELECT EXISTS(event_id = ?) FROM processed_events          â”‚
â”‚     â†“                                                           â”‚
â”‚     IF YES â†’ Skip processing, commit offset, RETURN            â”‚
â”‚                                                                 â”‚
â”‚  2. Process business logic (in transaction)                    â”‚
â”‚     â†“                                                           â”‚
â”‚     - Update payment tables                                    â”‚
â”‚     - Call external APIs                                       â”‚
â”‚     - Send notifications                                       â”‚
â”‚                                                                 â”‚
â”‚  3. Mark as processed (same transaction)                       â”‚
â”‚     â†“                                                           â”‚
â”‚     INSERT INTO processed_events (event_id, ...)               â”‚
â”‚                                                                 â”‚
â”‚  4. Commit Kafka offset (only if transaction succeeds)         â”‚
â”‚     â†“                                                           â”‚
â”‚     acknowledgment.acknowledge()                               â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ—„ï¸ Database Schema

### Table: `processed_events`

```sql
CREATE TABLE processed_events (
    id              BIGINT PRIMARY KEY AUTO_INCREMENT,
    event_id        VARCHAR(100) NOT NULL UNIQUE,  -- From Kafka event payload
    topic           VARCHAR(255) NOT NULL,          -- Kafka topic
    event_type      VARCHAR(100),                   -- "PAYMENT_APPROVED", etc.
    partition       INT,                            -- Kafka partition (for debugging)
    offset          BIGINT,                         -- Kafka offset (for debugging)
    processed_at    TIMESTAMP NOT NULL,             -- When processed

    INDEX idx_event_id (event_id),                  -- For fast lookups
    INDEX idx_processed_at (processed_at)           -- For cleanup queries
);
```

**Key Design Decisions:**
- `event_id` is **UNIQUE** â†’ Database enforces idempotency
- Indexed on `event_id` â†’ Fast `EXISTS` queries
- Stores `partition` and `offset` â†’ Debugging and tracing
- Stores `processed_at` â†’ Data retention policies

## ğŸ”’ Transactional Guarantees

### Spring `@Transactional` Annotation

```java
@Transactional
public void handlePaymentApproved(PaymentApprovedEvent event, ...) {
    // 1. Check idempotency
    if (processedEventRepository.existsByEventId(event.eventId())) {
        acknowledgment.acknowledge();
        return;
    }

    // 2. Business logic
    processPaymentApproved(event);

    // 3. Mark as processed (SAME transaction)
    processedEventRepository.save(new ProcessedEvent(...));

    // 4. Commit offset (only if transaction commits)
    acknowledgment.acknowledge();
}
```

**Transaction Boundary:**
- âœ… Idempotency check + Business logic + Insert processed_events = **1 atomic transaction**
- âœ… If business logic fails â†’ Rollback (no idempotency record saved)
- âœ… If database commit fails â†’ No offset commit (Kafka will redeliver)
- âœ… If offset commit fails â†’ Next delivery caught by idempotency check

## ğŸ§ª Failure Scenarios & Behavior

| Scenario | What Happens | Result |
|----------|-------------|--------|
| **Event already processed** | Idempotency check returns `true` â†’ Skip processing, commit offset | âœ… No duplicate |
| **Deserialization failure** | Throw exception â†’ Retry 5x â†’ DLQ | âœ… Handled by DLQ |
| **Business logic fails** | Transaction rollback â†’ No idempotency record â†’ Offset not committed | âœ… Retry without duplicate |
| **Database failure during save** | Transaction rollback â†’ Offset not committed â†’ Kafka redelivers | âœ… Retry without duplicate |
| **Consumer crash AFTER DB commit, BEFORE offset commit** | On restart: Kafka redelivers â†’ Idempotency check catches it â†’ Skip processing | âœ… No duplicate |
| **Consumer crash AFTER offset commit** | Kafka moves to next message | âœ… Normal flow |
| **Manual retry from DLQ** | Idempotency check catches duplicate | âœ… Safe to retry |

## ğŸ“ Code Components

### 1. Entity: `ProcessedEvent`

**Location:** `src/main/java/com/mvbr/store/domain/model/ProcessedEvent.java`

```java
@Entity
@Table(name = "processed_events")
public class ProcessedEvent {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    @Column(name = "event_id", unique = true, nullable = false)
    private String eventId;

    // ... other fields
}
```

### 2. Repository: `ProcessedEventRepository`

**Location:** `src/main/java/com/mvbr/store/domain/repository/ProcessedEventRepository.java`

```java
@Repository
public interface ProcessedEventRepository extends JpaRepository<ProcessedEvent, Long> {

    boolean existsByEventId(String eventId);  // Idempotency check

    Optional<ProcessedEvent> findByEventId(String eventId);

    long deleteByProcessedAtBefore(Instant cutoffDate);  // Data retention
}
```

### 3. Consumer: `PaymentApprovedConsumer`

**Location:** `src/main/java/com/mvbr/store/infrastructure/messaging/consumer/PaymentApprovedConsumer.java`

**Key Changes:**
- Added `@Transactional` annotation
- Added idempotency check before processing
- Save `ProcessedEvent` after business logic
- Inject `ProcessedEventRepository`

## ğŸš€ Production Considerations

### âœ… Implemented

- âœ… **Exactly-once processing** (idempotency pattern)
- âœ… **Transactional consistency** (DB + Kafka in sync)
- âœ… **DLQ for poison messages** (already configured)
- âœ… **Retry with exponential backoff** (already configured)
- âœ… **Manual commit** (fine-grained offset control)

### âš ï¸ Recommended for Production

1. **Data Retention Policy**
   ```java
   @Scheduled(cron = "0 0 2 * * ?")  // Daily at 2 AM
   public void cleanupOldProcessedEvents() {
       Instant cutoff = Instant.now().minus(90, ChronoUnit.DAYS);
       long deleted = processedEventRepository.deleteByProcessedAtBefore(cutoff);
       log.info("Deleted {} old processed events", deleted);
   }
   ```

2. **Monitoring**
   - Track `processed_events` table size
   - Alert if table grows too fast (indicates issues)
   - Monitor DLQ topic for poison messages

3. **Database Indexing**
   - Verify `idx_event_id` is being used (run `EXPLAIN` on queries)
   - Consider partitioning `processed_events` table if very large

4. **Performance Tuning**
   - Current implementation does 1 `EXISTS` query per message
   - For very high throughput, consider batch processing with in-memory cache

## ğŸ§° How to Test

### Test 1: Normal Processing

```bash
# Send event
curl -X POST http://localhost:5050/api/payments/approved \
  -H "Content-Type: application/json" \
  -d '{
    "paymentId": "pay-123",
    "userId": "user-456",
    "amount": 100.00
  }'

# Check H2 console
http://localhost:5050/h2-console
# SQL: SELECT * FROM processed_events WHERE event_id = ?
```

**Expected:** Event processed once, record in `processed_events`.

### Test 2: Duplicate Delivery

```bash
# Send same event twice (simulate duplicate)
curl -X POST ... (same payload)

# Check logs
# Expected: Second message skipped with "IDEMPOTENCY: Event already processed"
```

### Test 3: Consumer Crash Simulation

```bash
# 1. Send event
# 2. Kill consumer BEFORE offset commit (stop app mid-processing)
# 3. Restart consumer
# Expected: Message reprocessed, idempotency check catches duplicate
```

## ğŸ“Š Performance Impact

### Overhead per Message

- 1 additional `SELECT EXISTS` query (~1ms on indexed table)
- 1 additional `INSERT` query (~2ms)
- **Total overhead: ~3ms per message**

### Throughput

- With `MAX_POLL_RECORDS=1` (critical consumer): **~330 messages/second**
- Acceptable for critical payment flows
- For higher throughput, use batch processing

## ğŸ”— Related Configuration

- **Consumer Config:** `KafkaConsumerConfig.criticalKafkaListenerContainerFactory()`
  - Manual commit enabled
  - DLQ configured
  - Retry with exponential backoff

- **Database Config:** `application.yaml`
  - H2 in-memory for development
  - JPA auto-DDL enabled
  - SQL logging enabled

## ğŸ“š References

- [Kafka Exactly-Once Semantics](https://kafka.apache.org/documentation/#semantics)
- [Spring Kafka Idempotent Consumer](https://docs.spring.io/spring-kafka/reference/kafka/receiving-messages/message-listeners.html)
- [Idempotency Pattern](https://martinfowler.com/articles/patterns-of-distributed-systems/idempotent-receiver.html)

---

**Created:** 2025-12-05
**Last Updated:** 2025-12-05
**Status:** âœ… Production-ready